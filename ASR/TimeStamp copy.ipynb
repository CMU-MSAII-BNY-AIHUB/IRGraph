{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from xml.etree import ElementTree as ET\n",
    "import nltk\n",
    "import warnings\n",
    "import numpy as np\n",
    "import re\n",
    "import pytz\n",
    "from datetime import datetime,timedelta\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import yfinance as yf\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e:\\BNY Mellon capstone project\\BKG\n"
     ]
    }
   ],
   "source": [
    "from configparser import ConfigParser\n",
    "from pathlib import Path\n",
    "BASE_DIR =Path.cwd().parent\n",
    "print(BASE_DIR)\n",
    "CONFIG = ConfigParser()\n",
    "CONFIG.read(BASE_DIR / \"config.ini\")\n",
    "POLYGON_KEY = CONFIG.get(\"UPSTREAM\", \"polygon_api_key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"recording\"\n",
    "audio_file = \"The Bank of New York Mellon Corporation (NYSE_BK) Apr-16-2024 - Audio.mp3\"\n",
    "store_path = \"S2T\"\n",
    "store_S2T = \"S2T/BK-Q1-2024-S2T.json\"\n",
    "xml_path = \"xml\"\n",
    "# xml_file = \"BK-Q1-2024.xml\"\n",
    "xml_file = \"BK-Q1-2024.xml\"\n",
    "output_file = \"BK-Q1-2024_timestamp.xml\"\n",
    "stock_folder = \"stock\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 加载模型\n",
    "model = whisper.load_model(\"base\")\n",
    "\n",
    "# 进行语音识别\n",
    "result = model.transcribe(os.path.join(audio_path, audio_file))\n",
    "\n",
    "# # 打印结果\n",
    "for segment in result[\"segments\"]:\n",
    "    print(f\"Start: {segment['start']}s, End: {segment['end']}s, Text: {segment['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(os.path.join(store_path,store_S2T), 'w') as f:\n",
    "    json.dump(result, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.txt', 'w') as file:\n",
    "    json.dump(result[\"text\"], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(store_S2T, 'r') as file:\n",
    "    result = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e on the BNY Mallon investor relations website at two Eastern standard time today. Have a great day.\n"
     ]
    }
   ],
   "source": [
    "print(result[\"text\"][-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment in result[\"segments\"]:\n",
    "    print(f\"Start: {segment['start']}s, End: {segment['end']}s, Text: {segment['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best match found:  and welcome to State Street Corporation\n"
     ]
    }
   ],
   "source": [
    "import difflib\n",
    "\n",
    "def find_best_match(original_text, transcribed_segments):\n",
    "    matcher = difflib.SequenceMatcher(None, original_text, transcribed_segments)\n",
    "    match = matcher.find_longest_match(0, len(original_text), 0, len(transcribed_segments))\n",
    "    return original_text[match.a: match.a + match.size]\n",
    "\n",
    "\n",
    "original_text = \"Good morning, and welcome to State Street Corporation's First Quarter 2024 Earnings Conference Call and Webcast. Today's discussion is being broadcasted live on State Street's website at investors.statestreet.com. This conference call is also being recorded for replay. State Street's conference call is copyrighted, and all rights are reserved. This call may not be recorded or rebroadcast or distribution in whole or in part without the expressed written authorization from State Street Corporation. The only authorized broadcast of this call will be housed on the State Street website. Now I would like to introduce Ilene Fiszel Bieler, Global Head of Investor Relations at State Street. Please go ahead.\"\n",
    "transcribed_segments = \" Good morning and welcome to State Street Corporations, first quarter 2024 earnings conference\"\n",
    "\n",
    "best_match = find_best_match(original_text, transcribed_segments)\n",
    "print(\"Best match found:\", best_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_time(time_str):\n",
    "    \n",
    "    return datetime.strptime(time_str, '%A, %B %d, %Y %I:%M %p %Z')\n",
    "\n",
    "def timeAndTicker(root):\n",
    "    time_element = root.find(\".//header/time\")\n",
    "    if time_element is not None:\n",
    "        time_element = parse_time(time_element.text)\n",
    "    ticker =  root.find(\".//header/ticker\").text\n",
    "    return time_element, ticker\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    import re\n",
    "    return re.sub(r'\\W+', ' ', text.lower()).strip()\n",
    "\n",
    "def calculate_similarity(text1, text2):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf = vectorizer.fit_transform([text1, text2])\n",
    "    return cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]\n",
    "def split_text_into_sentences(text):\n",
    "\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    return tokenizer.tokenize(text)\n",
    "def find_most_similar_sentence(sentence, segments):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    max_similarity = 0\n",
    "    best_segment = None\n",
    "    best_index = -1\n",
    "    target_length = len(preprocess_text(sentence))\n",
    "\n",
    "    \n",
    "    for start_index in range(len(segments)):\n",
    "        combined_text = \"\"\n",
    "        for end_index in range(start_index, len(segments)):\n",
    "            combined_text += \" \" + segments[end_index]['text']\n",
    "            combined_length = len(preprocess_text(combined_text))\n",
    "\n",
    "\n",
    "            if combined_length < target_length * 1.5 or (target_length< 10 and combined_length < target_length * 56):  # 允许一定的长度超出\n",
    "                processed_combined_text = preprocess_text(combined_text)\n",
    "                all_texts = [preprocess_text(sentence), processed_combined_text]\n",
    "                tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "                similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
    "                # print(similarity)\n",
    "               \n",
    "                if similarity > max_similarity:\n",
    "                    max_similarity = similarity\n",
    "                    best_segment = segments[end_index]\n",
    "                    best_index = end_index\n",
    "            else:\n",
    "                break  \n",
    "    \n",
    "    return best_segment, max_similarity, best_index\n",
    "def get_last_two_sentences(text):\n",
    "\n",
    "    sentences = split_text_into_sentences(text)\n",
    "    # Combine the last two sentences, handle texts with less than two sentences.\n",
    "    last_two_sentences = \" \".join(sentences[-2:]) if len(sentences) >= 2 else \" \".join(sentences)\n",
    "    return last_two_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_gmt_to_et(gmt_time):\n",
    "    \n",
    "    print(f\"gmt_time: {gmt_time}\") \n",
    "    if gmt_time.tzinfo is None or gmt_time.tzinfo.utcoffset(gmt_time) is None:\n",
    "        gmt_timezone = pytz.timezone('GMT')\n",
    "        gmt_time = gmt_timezone.localize(gmt_time)  # 给 naive datetime 设置 GMT 时区\n",
    "\n",
    "    et_timezone = pytz.timezone('America/New_York')\n",
    "    et_time = gmt_time.astimezone(et_timezone)  # 转换到 ET 时区\n",
    "    print(f\"et_time: {et_time}\")  \n",
    "    return et_time\n",
    "\n",
    "def load_daily_stock_data(ticker, date):\n",
    "\n",
    "    start_date = date\n",
    "    end_date = pd.to_datetime(date).date() + pd.Timedelta(days=1)\n",
    "    data = yf.download(ticker, start=start_date, end=end_date, interval='1m')\n",
    "    return data\n",
    "def get_specific_data(specific_time, stock_data):\n",
    "    specific_time = pd.to_datetime(specific_time)  # 确保这是 datetime 对象\n",
    "    print(specific_time)\n",
    "    specific_data = None\n",
    "    if specific_time in stock_data.index:\n",
    "        specific_data = stock_data.loc[specific_time]\n",
    "        print(specific_data)\n",
    "    else:\n",
    "        print(\"No data available for the specified time.\")\n",
    "    return specific_data\n",
    "\n",
    "def round_time_to_nearest_minute(dt):\n",
    "\n",
    "    # 四舍五入的逻辑：如果秒数大于等于30，增加一分钟\n",
    "    new_minute = dt + timedelta(minutes=1) if dt.second >= 30 else dt\n",
    "    return new_minute.replace(second=0, microsecond=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_time = []\n",
    "global_price = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_presentation_stockprice_to_xml(root, time, stock_data, SP500_data, KBW_data, result):\n",
    "    \"\"\"\n",
    "    Add summaries to the XML file based on the section presentation.\n",
    "    \n",
    "    Args:\n",
    "        root: ElementTree of the transcript\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"processing presentation section\")\n",
    "    # Assume result is accessible and contains the whisper_segments\n",
    "    whisper_segments = result[\"segments\"]\n",
    "    prev_time = None\n",
    "    for statement_element in root.findall(\".//statement\"):\n",
    "        speaker_element = statement_element.find(\"speaker\")\n",
    "        text_element = speaker_element.find(\"text\")\n",
    "        text = text_element.text.strip()\n",
    "\n",
    "        last_sentence = get_last_two_sentences(text)\n",
    "\n",
    "        best_segment, best_similarity, best_index = find_most_similar_sentence(last_sentence, whisper_segments)\n",
    "\n",
    "        # Print the best matching segment's text, similarity score, and times\n",
    "        # print(\"Target sentence: \", last_sentence)\n",
    "        # print(\"Best Matching Segment:\", best_segment['text'])\n",
    "        # print(\"Similarity Score:\", best_similarity)\n",
    "        # print(f\"Start Time: {best_segment['start']}s, End Time: {best_segment['end']}s\")\n",
    "        \n",
    "        \n",
    "        end_time_gmt = time + timedelta(seconds=best_segment['end'])\n",
    "        rounded_end_time_gmt = round_time_to_nearest_minute(end_time_gmt)\n",
    "        end_time_et = convert_gmt_to_et(rounded_end_time_gmt)\n",
    "        stock_time_str = end_time_et.strftime('%Y-%m-%d %H:%M:%S')\n",
    "        stock_price = get_specific_data(stock_time_str, stock_data)\n",
    "        SP500 = get_specific_data(stock_time_str, SP500_data)\n",
    "        KBW = get_specific_data(stock_time_str, KBW_data)\n",
    "        # print(f\"Global time: {end_time_gmt}\")\n",
    "        # print(f\"stock_data: \", stock_data)\n",
    "        # print(stock_data)\n",
    "        # print(\"----------------------------------------------------\")\n",
    "        timestamp_element = ET.SubElement(text_element, \"timeStamp\")\n",
    "\n",
    "        timestamp_element.text = end_time_gmt.strftime('%H:%M:%S')\n",
    "        stock_element = ET.SubElement(text_element, \"stock_price\")\n",
    "        stock_element.text = f\"{stock_price['Close']:.6f}\"\n",
    "        S_P500_element = ET.SubElement(text_element, \"S_P500\")\n",
    "        \n",
    "        S_P500_element.text = f\"{SP500['Close']:.6f}\" \n",
    "        KBW_element = ET.SubElement(text_element, \"KBW\")\n",
    "        KBW_element.text = f\"{KBW['Close']:.6f}\"\n",
    "        global_time.append(end_time_et)\n",
    "        global_price.append(stock_price)\n",
    "\n",
    "        # if \"[\" in text:\n",
    "        #     print(\"\\nTrigger\\n\")\n",
    "        #     ending_text = text.split(']')[-1]\n",
    "        #     sentence = split_text_into_sentences(ending_text)[0]\n",
    "        #     # print(f\"ending text: {sentence}\")\n",
    "        #     best_segment, best_similarity,max_index = find_most_similar_sentence(sentence, whisper_segments[i:])\n",
    "        #     # print(\"Best Matching Segment:\", best_segment['text'])\n",
    "        #     # print(\"Similarity Score:\", best_similarity)\n",
    "        #     # print(whisper_segments[max_index][\"text\"], whisper_segments[i+max_index][\"text\"])\n",
    "        #     i = i+max_index\n",
    "        #     start_index = i\n",
    "        #     processed_paragraph = preprocess_text(ending_text)\n",
    "\n",
    "        # while i < len(whisper_segments):\n",
    "        #     current_text += \" \" + whisper_segments[i]['text']\n",
    "        #     new_processed_text = preprocess_text(current_text)\n",
    "        #     similarity = calculate_similarity(processed_paragraph, new_processed_text)\n",
    "        #     print(similarity)\n",
    "        #     if similarity > max_similarity:\n",
    "        #         max_similarity = similarity\n",
    "        #         best_match_text = current_text\n",
    "        #         i += 1\n",
    "                \n",
    "        #     elif similarity > 0.7 and similarity < max_similarity and abs(len(current_text) - len(processed_paragraph))< 30:\n",
    "        #         start_index = i\n",
    "        #         break\n",
    "        #     else:\n",
    "        #         i +=1\n",
    "        # print(f\"origin Text: {processed_paragraph}\")\n",
    "        # print(f\"Matched Text: {best_match_text}\")\n",
    "        # print(f\"Start Time: {whisper_segments[start_index]['start']}s, End Time: {whisper_segments[i]['end']}s\")    \n",
    "        # print(\"----------------------------------------------------\")\n",
    "\n",
    "        # # Use summarizer to get the summary for the text\n",
    "        # summary = self.summarizer.summarize(text, \"statement\")\n",
    "        # # Create and append the summary tag\n",
    "        # summary_element = ET.SubElement(text_element, \"summary\")\n",
    "        # summary_element.text = summary\n",
    "    return root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_QA_stockprice_to_xml(root, time, stock_data, SP500_data, KBW_data,result):\n",
    "        \"\"\"\n",
    "        Add summaries to the XML file based on the section Question and Answer.\n",
    "        \n",
    "        Args:\n",
    "            root: ElementTree of the transcript\n",
    "        \"\"\"\n",
    "        print(\"processing QA section\")\n",
    "        whisper_segments = result[\"segments\"]\n",
    "        # Find the <section name=\"Question and Answer\"> section\n",
    "        qa_section = root.find(\"./body/section[@name='Question and Answer']\")\n",
    "\n",
    "        # Iterate over the elements within the section \n",
    "        text_type = \"\"\n",
    "        prev_time = None\n",
    "        for element in qa_section.iter():\n",
    "            \n",
    "            # \n",
    "            if element.tag == \"transition\" or element.tag==\"ending\":\n",
    "                text_type = \"transition\"\n",
    "                \n",
    "            elif \"question\" in element.tag.lower():\n",
    "                text_type = \"question\"\n",
    "                \n",
    "            elif \"answer\" in element.tag.lower():\n",
    "                text_type = \"answer\"\n",
    "                \n",
    "                \n",
    "\n",
    "            if element.tag == 'text':\n",
    "\n",
    "                if text_type == \"question\":\n",
    "\n",
    "                    text = \"\"\n",
    "                    if element.text is None:\n",
    "                        print(\"There is None text in Q&A\")\n",
    "                        print(\"_________________________________________\")\n",
    "                    else:\n",
    "                        text = element.text.strip()\n",
    "                    \n",
    "\n",
    "                    last_sentence = get_last_two_sentences(text)\n",
    "\n",
    "                    best_segment, best_similarity, best_index = find_most_similar_sentence(last_sentence, whisper_segments)\n",
    "\n",
    "                    # Print the best matching segment's text, similarity score, and times\n",
    "                    print(\"Target sentence: \", last_sentence)\n",
    "                    # print(\"Best Matching Segment:\", best_segment['text'])\n",
    "                    # print(\"Similarity Score:\", best_similarity)\n",
    "                    # print(f\"Start Time: {best_segment['start']}s, End Time: {best_segment['end']}s\")\n",
    "                    if best_segment == None:\n",
    "                        timestamp_element = ET.SubElement(element, \"timeStamp\")\n",
    "                        timestamp_element.text = \"None\"\n",
    "                        stock_element = ET.SubElement(element, \"stock_price\")\n",
    "                        stock_element.text = \"None\"\n",
    "                        continue\n",
    "                     \n",
    "                    end_time_gmt = time + timedelta(seconds=best_segment['end'])\n",
    "                    rounded_end_time_gmt = round_time_to_nearest_minute(end_time_gmt)\n",
    "                    end_time_et = convert_gmt_to_et(rounded_end_time_gmt)\n",
    "                    stock_time_str = end_time_et.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    stock_price = get_specific_data(stock_time_str, stock_data)\n",
    "                    SP500 = get_specific_data(stock_time_str, SP500_data)\n",
    "                    KBW = get_specific_data(stock_time_str, KBW_data)\n",
    "                    \n",
    "                    timestamp_element = ET.SubElement(element, \"timeStamp\")\n",
    "                    timestamp_element.text = end_time_gmt.strftime('%H:%M:%S')\n",
    "                    stock_element = ET.SubElement(element, \"stock_price\")\n",
    "                    stock_element.text = f\"{stock_price['Close']:.6f}\"\n",
    "                    S_P500_element = ET.SubElement(element, \"S_P500\")\n",
    "                    S_P500_element.text = f\"{SP500['Close']:.6f}\" \n",
    "                    KBW_element = ET.SubElement(element, \"KBW\")\n",
    "                    KBW_element.text = f\"{KBW['Close']:.6f}\"\n",
    "                    global_time.append(end_time_et)\n",
    "                    global_price.append(stock_price)\n",
    "\n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(os.path.join(xml_path,xml_file))\n",
    "root = tree.getroot()\n",
    "time, ticker = timeAndTicker(root)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmt_time: 2024-04-16 15:00:00\n",
      "et_time: 2024-04-16 11:00:00-04:00\n",
      "2024-04-16 11:00:00-04:00\n",
      "gmt_time: 2024-04-16 15:00:00\n",
      "et_time: 2024-04-16 11:00:00-04:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(convert_gmt_to_et(time))\n",
    "stock_data = load_daily_stock_data(ticker,convert_gmt_to_et(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_data = load_daily_stock_data(\"^GSPC\",convert_gmt_to_et(time))\n",
    "BKX_data = load_daily_stock_data(\"^BKX\",convert_gmt_to_et(time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse(os.path.join(xml_path,xml_file))\n",
    "root = tree.getroot()\n",
    "time, ticker = timeAndTicker(root)\n",
    "print(time, ticker)\n",
    "root = add_presentation_stockprice_to_xml(root,time,stock_data, SP500_data, BKX_data, result)\n",
    "root = add_QA_stockprice_to_xml(root, time, stock_data, SP500_data, BKX_data,result)\n",
    "tree.write(output_file, encoding='utf-8', xml_declaration=True)\n",
    "print(f\"Processed {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_stock_data(stock_data):\n",
    "    \"\"\"\n",
    "    Plots the stock data for open, close, high, and low prices.\n",
    "\n",
    "    Args:\n",
    "        stock_data (DataFrame): DataFrame containing the stock data with datetime index.\n",
    "    \"\"\"\n",
    "    # Check if data is empty\n",
    "    if stock_data.empty:\n",
    "        print(\"No data available to plot.\")\n",
    "        return\n",
    "\n",
    "    # Ensure the index is a datetime index\n",
    "    stock_data.index = pd.to_datetime(stock_data.index)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(stock_data.index, stock_data['Open'], label='Open', color='green')\n",
    "    plt.plot(stock_data.index, stock_data['Close'], label='Close', color='red')\n",
    "    plt.plot(stock_data.index, stock_data['High'], label='High', color='black')\n",
    "    plt.plot(stock_data.index, stock_data['Low'], label='Low', color='blue')\n",
    "\n",
    "    plt.title('Stock Prices')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)  # Rotate dates for better visibility\n",
    "    plt.tight_layout()  # Adjust subplots to give some padding\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'stock_data' is your DataFrame loaded with stock prices\n",
    "plot_stock_data(stock_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_and_sort_dataframe(global_times, global_prices):\n",
    "    df = pd.DataFrame({\n",
    "        'Time': global_times,\n",
    "        'Price': [price['Close'] for price in global_prices]  \n",
    "    })\n",
    "    df['Time'] = pd.to_datetime(df['Time']) \n",
    "    df.sort_values('Time', inplace=True)  \n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_stock_prices(df):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(df['Time'], df['Price'], marker='o', linestyle='-', color='b')\n",
    "    plt.title('Stock Prices Over Time')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df = create_and_sort_dataframe(global_time, global_price)\n",
    "plot_stock_prices(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data.to_csv(os.path.join(stock_folder, xml_file.replace(\"xml\",\"csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_data.to_csv(os.path.join(stock_folder, xml_file.replace(\".xml\",\"-SP500.csv\")))\n",
    "BKX_data.to_csv(os.path.join(stock_folder, xml_file.replace(\".xml\",\"-KBW.csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
